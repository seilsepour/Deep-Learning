{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "f1WMHeeODQfr",
        "Sc3PlYP6NXlq",
        "Sg_UWHAkz66o"
      ],
      "authorship_tag": "ABX9TyOj5VYxcjlE/g9/sUx3wTVO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seilsepour/Deep-Learning/blob/master/Opinion_Words_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Adding Negation\n",
        "2.   Adding Modal\n",
        "\n"
      ],
      "metadata": {
        "id": "avptGqXrMUjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Dec  8 01:48:29 2019\n",
        "@author: russell\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "import spacy\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "nlp_english = spacy.load('en')\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "dic = {}"
      ],
      "metadata": {
        "id": "Zp5jCTTnmzbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COLAB"
      ],
      "metadata": {
        "id": "f1WMHeeODQfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/MYDRIVE', force_remount=True)"
      ],
      "metadata": {
        "id": "MVsMBf5pBxNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance"
      ],
      "metadata": {
        "id": "Sc3PlYP6NXlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pycm"
      ],
      "metadata": {
        "id": "U8Ha1ewCOr--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Jan  9 18:47:48 2019\n",
        "@author: russell\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from pycm import ConfusionMatrix\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "import gzip\n",
        "import gensim\n",
        "import logging\n",
        "\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
        "from nltk import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "class TF_IDF(object):\n",
        "    def __init__(self):\n",
        "        print(\"..\")\n",
        "\n",
        "    def get_tf_idf(self, X):\n",
        "        vectorizer = TfidfVectorizer(ngram_range=(1,2), tokenizer=lambda x: x.split())\n",
        "        X = vectorizer.fit_transform(X)\n",
        "        #print(X)\n",
        "        return X\n",
        "\n",
        "\n",
        "class Performance(object):\n",
        "\n",
        "\n",
        "    def get_results(self, labels, predictions):\n",
        "        from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "        conf_matrix = confusion_matrix(labels, predictions)\n",
        "\n",
        "        #print(conf_matrix)\n",
        "\n",
        "        precision = np.diag(conf_matrix) / np.sum(conf_matrix, axis = 0)\n",
        "        recall = np.diag(conf_matrix) / np.sum(conf_matrix, axis = 1)\n",
        "\n",
        "        accuracy =  np.sum(np.diag(conf_matrix) / np.sum(conf_matrix))\n",
        "\n",
        "        #print(\"Total: \" , np.sum(conf_matrix))\n",
        "\n",
        "        #print(\"Correct/Incorrect : \", np.sum(np.diag(conf_matrix) ),  np.sum(conf_matrix) -  np.sum(np.diag(conf_matrix) ))\n",
        "        #print(\"Denominator:\", np.sum(conf_matrix, axis = 0))\n",
        "        #print(np.mean(precision))\n",
        "        #print(np.mean(recall))\n",
        "\n",
        "        precision = np.mean(precision)\n",
        "        recall = np.mean(recall)\n",
        "\n",
        "        #print(\"Precision: \", precision)\n",
        "        #print(\"Recall: \" , recall)\n",
        "\n",
        "\n",
        "        f1_score = (2 * precision * recall) / (precision + recall)\n",
        "        #print(\"F-1 Score:  -----  \", f1_score)\n",
        "        return conf_matrix, precision,  recall, f1_score,  accuracy\n",
        "\n",
        "\n",
        "    def calculateMCC(self, labels, predictions):\n",
        "\n",
        "        cm = ConfusionMatrix(labels, predictions, digit=5)\n",
        "\n",
        "        #print(\"\\n Kappa-AC1: \", cm.Kappa, cm.AC1)\n",
        "        #print(\"\\nMCC:\")\n",
        "        #print(cm.MCC)\n",
        "        return cm.MCC\n",
        "\n",
        "\n",
        "    def calculate_roc_auc_score(self, labels, predictions):\n",
        "        score = roc_auc_score(labels, predictions)\n",
        "        return score\n",
        "        #print(\"ROC:\" , score)"
      ],
      "metadata": {
        "id": "Em4iAW5lCTjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LexicalAnalyzer"
      ],
      "metadata": {
        "id": "i28vm6IInYOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LexicalAnalyzer(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.create_polarity_dictionary_opinion_lexicon()\n",
        "\n",
        "\n",
        "    def read_data(self):\n",
        "\n",
        "        data = pd.read_csv(\"/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/Pang-Ver2-00-10-24.csv\", header=None) #text in column 1, classifier in column 2.\n",
        "        numpy_array = data.values\n",
        "        X = numpy_array[:,2]\n",
        "        Y = numpy_array[:,1]\n",
        "\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "\n",
        "    def write_CSV(self, X_data, Y_label, Y_prediction, confidence_scores ,category):\n",
        "\n",
        "        csvfile=open(\"/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/\" + category +\".csv\",'w', newline='')\n",
        "\n",
        "\n",
        "        obj=csv.writer(csvfile)\n",
        "\n",
        "        data= []\n",
        "        label = []\n",
        "        prediction = []\n",
        "        confidence = []\n",
        "        for i in range(len(X_data)):\n",
        "            data.append(X_data[i])\n",
        "            label.append(Y_label[i])\n",
        "            prediction.append( Y_prediction[i])\n",
        "            confidence.append(confidence_scores[i])\n",
        "\n",
        "        for element in zip(data, label,prediction, confidence):\n",
        "            obj.writerow(element)\n",
        "\n",
        "        csvfile.close()\n",
        "\n",
        "    def write_opinion_words(self, X_data, positive_opinion_words, positive_negation_words, negative_opinion_words,  negative_negation_words, Y_label, Y_prediction, confidence_scores ):\n",
        "\n",
        "        csvfile = open(\"/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/Opinion_words.csv\",'w', newline='')\n",
        "\n",
        "\n",
        "        obj=csv.writer(csvfile)\n",
        "\n",
        "        data= []\n",
        "        label = []\n",
        "        prediction = []\n",
        "        confidence = []\n",
        "        positives = []\n",
        "        negatives = []\n",
        "        positive_negations = []\n",
        "        negative_negations = []\n",
        "        for i in range(len(X_data)):\n",
        "            data.append(X_data[i])\n",
        "            label.append(Y_label[i])\n",
        "            prediction.append( Y_prediction[i])\n",
        "            confidence.append(confidence_scores[i])\n",
        "            positives.append(positive_opinion_words[i])\n",
        "            negatives.append(negative_opinion_words[i])\n",
        "            positive_negations.append(positive_negation_words[i])\n",
        "            negative_negations.append(negative_negation_words[i])\n",
        "\n",
        "\n",
        "\n",
        "        for element in zip(data, positives, positive_negations, negatives, negative_negations, label,prediction, confidence):\n",
        "            obj.writerow(element)\n",
        "\n",
        "        csvfile.close()\n",
        "\n",
        "\n",
        "    def preprocess_data(self,text):\n",
        "\n",
        "        text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "        text = text.strip().replace(\".\", \".\").replace(\".\", \".\")\n",
        "        text = text.lower()\n",
        "        return text\n",
        "\n",
        "\n",
        "    def stem_data(self, text):\n",
        "        ps = PorterStemmer()\n",
        "        modified_text = \" \".join(ps.stem(w)for w in nltk.wordpunct_tokenize(text))\n",
        "\n",
        "        return modified_text\n",
        "\n",
        "    def split_review_text(self,review):\n",
        "        import re\n",
        "        split_sentences = review.split(\"�\")\n",
        "        split_sentences  = re.split('[.,]', review) # re.split('[^a-zA-Z][]', review)\n",
        "        sentences = []\n",
        "\n",
        "        for s in split_sentences:\n",
        "            if len(s) > 1:\n",
        "                sentences.append(s)\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    def remove_pronoun(self,tokens):\n",
        "        pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'what', 'who','me', 'him', 'her', 'it', 'us', 'you', 'them', 'whom','mine', 'yours', 'his', 'hers', 'ours', 'theirs','this', 'that', 'these', 'those']\n",
        "\n",
        "        for token in tokens:\n",
        "            for pronoun in pronouns:\n",
        "                if token == pronoun:\n",
        "                    #print(\"^^^^^^^^\\:   \", token, pronoun)\n",
        "                    tokens.remove(token)\n",
        "                    break\n",
        "\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    #----------- Find adjective and adverb --------------\n",
        "    def get_adjective(self, tokens):\n",
        "\n",
        "        text = ' '.join(tokens)\n",
        "\n",
        "        tokens = nlp_english(text)\n",
        "        for token in tokens:\n",
        "            if token.pos_ == 'ADJ' or token.pos_ == 'ADV':\n",
        "                return token\n",
        "\n",
        "        return ''\n",
        "\n",
        "\n",
        "    #----------- Check whether text contains adjective and adverb --------------\n",
        "    def does_contain_adjective(self, tokens):\n",
        "\n",
        "        text = ' '.join(tokens)\n",
        "\n",
        "        tokens = nlp_english(text)\n",
        "        for token in tokens:\n",
        "            if token.pos_ == 'ADJ' or token.pos_ == 'ADV':\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    #----------- Check polarity Shifter due to Negation  --------------\n",
        "    def get_negation_score(self,tokens):\n",
        "\n",
        "        text = ' '.join(tokens)\n",
        "\n",
        "        tokens = nlp_english(text)\n",
        "        for i in range(len(tokens) - 2):\n",
        "            if tokens[i].dep_ == 'neg' and  (tokens[i + 1].pos_ == 'ADJ' or tokens[i + 2].pos_ == 'ADJ' ):# or token.text == 'should' or token.text == 'could' or token.text == 'must' :\n",
        "\n",
        "               if tokens[i + 1].pos_ == 'ADJ':\n",
        "                   token = tokens[i + 1].text\n",
        "               else:\n",
        "                   token =  tokens[i + 2].text\n",
        "\n",
        "               if token in  dic.keys():\n",
        "                   return  - 2 * dic[token]\n",
        "\n",
        "        #not good, do not like, not terrible\n",
        "        for i in range(len(tokens) - 1):\n",
        "             if tokens[i].dep_ == 'neg' and  (tokens[i + 1].pos_ == 'ADJ'):# or token.text == 'should' or token.text == 'could' or token.text == 'must' :\n",
        "\n",
        "                token = tokens[i + 1].text\n",
        "\n",
        "                if token in  dic.keys():\n",
        "                   #print(\"^^^^^ ^^^ ^ ^^  ^^ \" , token, dic[token])\n",
        "                   return  - 2 * dic[token]\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #----------- Check presence of Comparison in sentence  --------------\n",
        "    def get_comparison_score(self,tokens):\n",
        "        text = ' '.join(tokens)\n",
        "        #print(text)\n",
        "        tokens = nlp_english(text)\n",
        "        for i in range(len(tokens) - 2 ):\n",
        "            token = tokens[i]\n",
        "            next_token = tokens[i + 2]\n",
        "            #could/should/must be ?,  negate the ?\n",
        "            if token.text == 'should' or token.text == 'could' or token.text == 'must' :\n",
        "                if next_token in dic.keys():\n",
        "                    return   dic[next_token] * -1\n",
        "        return 0\n",
        "\n",
        "\n",
        "    #----------- Bing Liu opinion Lexicon --------------\n",
        "    def create_polarity_dictionary_opinion_lexicon(self):\n",
        "        file = open('/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/positive.txt', 'r')\n",
        "        for line in file:\n",
        "            token = line.split()\n",
        "            key = ''.join(token)\n",
        "            dic[key] = 1\n",
        "\n",
        "        file = open('/MYDRIVE/My Drive/Colab Notebooks/SecondPaper/data/negative.txt', 'r')\n",
        "        for line in file:\n",
        "            token = line.split()\n",
        "            key = ''.join(token)\n",
        "            dic[key] = -1\n",
        "\n",
        "\n",
        "    def get_polarity_score(self, aspect_sentence):\n",
        "        total_sum = 0\n",
        "        positive_score = 0\n",
        "        negative_score = 0\n",
        "        ddd = 0\n",
        "        for token in aspect_sentence:\n",
        "            if token in dic.keys():\n",
        "                total_sum += dic[token]\n",
        "                if dic[token] == 1:\n",
        "                    positive_score += dic[token]\n",
        "                elif dic[token] == -1:\n",
        "                    negative_score  -= dic[token]\n",
        "                else:\n",
        "                    ddd = 0\n",
        "\n",
        "        return total_sum, positive_score, negative_score\n",
        "\n",
        "\n",
        "\n",
        "    def remove_text_index(self,text):\n",
        "\n",
        "        text = text.strip()\n",
        "        index = text.find(\":\")\n",
        "        text = text [index + 1:]\n",
        "\n",
        "        return text\n",
        "\n",
        "    #----------- Find Opinion by Seilsepour  --------------\n",
        "    def get_opinion_words(self, aspect_sentence):\n",
        "        total_sum = 0\n",
        "        positive_score = 0\n",
        "        positive_words = list()\n",
        "        negative_words = list()\n",
        "        negative_score = 0\n",
        "        ddd = 0\n",
        "        for token in aspect_sentence:\n",
        "            if token in dic.keys():\n",
        "                total_sum += dic[token]\n",
        "                if dic[token] == 1:\n",
        "                    positive_score += dic[token]\n",
        "                    positive_words.append(token)\n",
        "                elif dic[token] == -1:\n",
        "                    negative_score  -= dic[token]\n",
        "                    negative_words.append(token)\n",
        "                else:\n",
        "                    ddd = 0\n",
        "\n",
        "        return positive_words, negative_words\n",
        "\n",
        "    #----------- Check polarity Shifter due to Negation by Seilsepour  --------------\n",
        "    def get_negation_words(self,tokens, ):\n",
        "        #print(\"get_negation_words\")\n",
        "        positive_negation_words = list()\n",
        "        negative_negation_words = list()\n",
        "\n",
        "        text = ' '.join(tokens)\n",
        "\n",
        "        tokens = nlp_english(text)\n",
        "        for i in range(len(tokens) - 2):\n",
        "            #print(\"for1\")\n",
        "            if tokens[i].dep_ == 'neg' and  (tokens[i + 1].pos_ == 'ADJ' or tokens[i + 2].pos_ == 'ADJ' ):# or token.text == 'should' or token.text == 'could' or token.text == 'must' :\n",
        "\n",
        "               if tokens[i + 1].pos_ == 'ADJ':\n",
        "                   token = tokens[i + 1].text\n",
        "               else:\n",
        "                   token =  tokens[i + 2].text\n",
        "               #print(\"token is \" + str(token))\n",
        "               if token in dic.keys():\n",
        "                  print(token)\n",
        "                  if dic[token] == 1:\n",
        "                    positive_negation_words.append(token)\n",
        "                  else:\n",
        "                    negative_negation_words.append(token)\n",
        "               #if token in  dic.keys():\n",
        "                   #return  - 2 * dic[token]\n",
        "\n",
        "        #not good, do not like, not terrible\n",
        "        for i in range(len(tokens) - 1):\n",
        "             if tokens[i].dep_ == 'neg' and  (tokens[i + 1].pos_ == 'ADJ'):# or token.text == 'should' or token.text == 'could' or token.text == 'must' :\n",
        "                #print(\"for2\")\n",
        "                #print(\"token is \" + str(tokens[i]))\n",
        "                token = tokens[i + 1].text\n",
        "                #print(\"next token is \" + str(tokens[i+1]))\n",
        "                if token in dic.keys():\n",
        "                   #print(token)\n",
        "                   if dic[token] == 1:\n",
        "                      positive_negation_words.append(token)\n",
        "                   else:\n",
        "                      negative_negation_words.append(token)\n",
        "\n",
        "                #if token in  dic.keys():\n",
        "                   #print(\"^^^^^ ^^^ ^ ^^  ^^ \" , token, dic[token])\n",
        "                   #return  - 2 * dic[token]\n",
        "        return positive_negation_words, negative_negation_words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #----------- Check presence of Comparison in sentence  --------------\n",
        "    def get_comparison_words(self,tokens):\n",
        "        #print(\"get_comparison_words\")\n",
        "        positive_negation_words = list()\n",
        "        negative_negation_words = list()\n",
        "        text = ' '.join(tokens)\n",
        "        #print(text)\n",
        "        tokens = nlp_english(text)\n",
        "        for i in range(len(tokens) - 2 ):\n",
        "            token = tokens[i]\n",
        "            next_token = tokens[i + 2]\n",
        "            #could/should/must be ?,  negate the ?\n",
        "            if token.text == 'should' or token.text == 'could' or token.text == 'must' :\n",
        "                #print(\"token is \" + str(token))\n",
        "                #print(\"next token is \" + str(next_token))\n",
        "                if next_token in dic.keys():\n",
        "                    #print(\"token is\" + str(token))\n",
        "                    #print(\"next token is\" + str(next_token))\n",
        "                    if dic[next_token] == 1:\n",
        "                       positive_negation_words.append(next_token)\n",
        "                    else:\n",
        "                       negative_negation_words.append(next_token)\n",
        "                    #return   dic[next_token] * -1\n",
        "        #return 0\n",
        "        return positive_negation_words, negative_negation_words\n",
        "\n",
        "    def classify_binary_dataset(self, X_data, Y_label):\n",
        "\n",
        "        num_of_detection = 0\n",
        "        true_prediction = 0\n",
        "        false_prediction = 0\n",
        "        positive_opinion_words = []\n",
        "        negative_opinion_words = []\n",
        "\n",
        "        positive_negation_words = []\n",
        "        negative_negation_words = []\n",
        "\n",
        "        prediction_confidence_scores = []\n",
        "\n",
        "        predictions = []\n",
        "        i = 0\n",
        "\n",
        "        Y_label = Y_label.astype('int')\n",
        "\n",
        "\n",
        "        for user_review in X_data:\n",
        "            #print(i, user_review)\n",
        "            if len(str(user_review)) < 5:\n",
        "                i += 1\n",
        "                prediction_confidence_scores. append(-1)\n",
        "                print(\"\\n\\n\\n\\n: Less: \",user_review, \"\\n\\n\\n\\n\" )\n",
        "                continue;\n",
        "\n",
        "            sentiments = []\n",
        "\n",
        "            user_review = self.preprocess_data(user_review)\n",
        "            user_review = self.split_review_text(user_review)\n",
        "\n",
        "\n",
        "            total_score = 0\n",
        "            total_aspect_term = 0\n",
        "            total_positive_score = 0\n",
        "            total_negative_score = 0\n",
        "\n",
        "            for sentence in user_review:\n",
        "                tokens = nlp_english(sentence)\n",
        "                #print(\">> \",tokens)\n",
        "\n",
        "                aspect_sentence = []\n",
        "\n",
        "\n",
        "                for token in tokens:\n",
        "                    # if not token.is_stop:\n",
        "                    #if  token.dep_ == 'nsubj' or  token.dep_ == 'amod' or token.pos_ == 'ADJ':\n",
        "                    if token.dep_ == 'nsubj' or token.dep_ == 'neg'  or token.dep_ == 'advmod' or token.dep_ == 'ROOT' or token.dep_ == 'compound' or token.pos_ == 'ADJ' or token.pos_ == 'NOUN' or token.text == 'could' or token.text == 'must' or token.text == 'should':\n",
        "                       #print(token.text, token.dep_,  token.pos_,  [child for child in token.children])\n",
        "                       aspect_sentence.append(token.text)\n",
        "\n",
        "                #print(\"aspect_sentence\",aspect_sentence)\n",
        "                if len(aspect_sentence) >= 2:\n",
        "                    num_of_detection += 1\n",
        "\n",
        "                    sentiments.append(aspect_sentence)\n",
        "\n",
        "                    aspect_sentence = self.remove_pronoun(aspect_sentence)\n",
        "\n",
        "\n",
        "                    if self.does_contain_adjective(aspect_sentence) == True:\n",
        "\n",
        "\n",
        "                        score, positive,negative = self.get_polarity_score(aspect_sentence)\n",
        "                        positives, negatives = self.get_opinion_words(aspect_sentence)\n",
        "                        positive_opinion_words.append(positives)\n",
        "                        negative_opinion_words.append(negatives)\n",
        "\n",
        "                        positive_negetions , negative_negations = self.get_negation_words(aspect_sentence)\n",
        "                        positive_negation_words.append(positive_negetions)\n",
        "                        negative_negation_words.append(negative_negations)\n",
        "\n",
        "\n",
        "                        positive_negetions , negative_negations = self.get_comparison_words(aspect_sentence)\n",
        "                        positive_negation_words.append(positive_negetions)\n",
        "                        negative_negation_words.append(negative_negations)\n",
        "\n",
        "                        #something here for negation words and comparison\n",
        "                        # by Seilsepour\n",
        "\n",
        "                        #print(\"-- \",i, score, positive,negative)\n",
        "\n",
        "                        total_positive_score += positive\n",
        "                        total_negative_score += negative\n",
        "\n",
        "                        negation_score = self.get_negation_score(aspect_sentence)\n",
        "                        if abs(negation_score) > 0 :\n",
        "                            score  +=  negation_score\n",
        "                            if negation_score < 0:\n",
        "                                total_positive_score -= 1\n",
        "                                total_negative_score += 1\n",
        "                            else:\n",
        "                                total_negative_score -= 1\n",
        "                                total_positive_score += 1\n",
        "\n",
        "                            #print (\"Neg: \",score,  aspect_sentence, negation_score)\n",
        "                        total_score += score\n",
        "                        total_score += self.get_comparison_score(aspect_sentence)\n",
        "                        total_negative_score -=  self.get_comparison_score(aspect_sentence)\n",
        "\n",
        "                total_aspect_term += len(aspect_sentence)\n",
        "\n",
        "\n",
        "            predicted_label = 0\n",
        "            true_label = int(Y_label[i])\n",
        "\n",
        "            if total_score >= 0:\n",
        "                predicted_label = 1\n",
        "\n",
        "\n",
        "            total_positive_negative = total_positive_score + total_negative_score\n",
        "\n",
        "            #print(i, total_positive_score, total_negative_score, total_positive_negative, total_score)\n",
        "\n",
        "            if total_score != 0:\n",
        "                #if total_positive_score >= total_negative_score:\n",
        "                prediction_confidence_score =  float(abs(total_positive_score - total_negative_score)/total_positive_negative)\n",
        "            else:\n",
        "                prediction_confidence_score = 0\n",
        "            prediction_confidence_scores.append(prediction_confidence_score)\n",
        "\n",
        "\n",
        "            if predicted_label == true_label:\n",
        "                true_prediction += 1\n",
        "            else:\n",
        "                false_prediction += 1\n",
        "                for aspect_sentence in sentiments:\n",
        "                    score = self.get_polarity_score(aspect_sentence)\n",
        "\n",
        "            predictions.append(predicted_label)\n",
        "\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        print(\"\\n!!!!!!\",len(Y_label), len(predictions))\n",
        "\n",
        "        #from SupervisedAlgorithm import  Performance\n",
        "        #print(\"Prediction ------\")\n",
        "        #performance = Performance()\n",
        "        #conf_matrix, f1_score, precision,  recall,acc = performance.get_results(Y_label, predictions)\n",
        "        #print(\"----\",round(f1_score,4), round(precision,4),  round(recall,4), round(acc,4) )\n",
        "\n",
        "        #print(conf_matrix)\n",
        "\n",
        "        return predictions, prediction_confidence_scores, positive_opinion_words, positive_negation_words, negative_opinion_words,  negative_negation_words\n",
        "\n",
        "\n",
        "    #----------Dataset----------\n",
        "    def distribute_predictions_into_bins(self, data, labels, predictions, confidence, positive_opinion_words, positive_negation_words, negative_opinion_words, negative_negation_words):\n",
        "\n",
        "        print(\"$$$$\")\n",
        "        n_conf = np.array(confidence)\n",
        "\n",
        "        thr = np.mean(n_conf) + 0.5 * np.std(n_conf)\n",
        "\n",
        "        print(np.mean(n_conf), thr)\n",
        "\n",
        "\n",
        "\n",
        "        bin1 = thr\n",
        "        bin2 = thr - 0.5 * np.std(n_conf)\n",
        "        bin3 = thr - np.std(n_conf)\n",
        "        bin4 = 0.0001\n",
        "\n",
        "        print(\"Bin Threshold\", bin1, bin2,bin3,bin4)\n",
        "\n",
        "        #return\n",
        "\n",
        "        very_high_data = []\n",
        "        very_high_label = []\n",
        "        very_high_prediction = []\n",
        "        very_high_confidence = []\n",
        "\n",
        "        high_data = []\n",
        "        high_label = []\n",
        "        high_prediction = []\n",
        "        high_confidence = []\n",
        "\n",
        "        low_data = []\n",
        "        low_label = []\n",
        "        low_prediction = []\n",
        "        low_confidence = []\n",
        "\n",
        "\n",
        "        very_low_data = []\n",
        "        very_low_label = []\n",
        "        very_low_prediction = []\n",
        "        very_low_confidence = []\n",
        "\n",
        "        zero_data = []\n",
        "        zero_label = []\n",
        "        zero_prediction = []\n",
        "        zero_confidence = []\n",
        "\n",
        "        print(\"---->>> \",len(data), len(confidence))\n",
        "        #return\n",
        "\n",
        "        for i in range(len(confidence)):\n",
        "            conf = confidence[i]\n",
        "            if conf >= bin1:\n",
        "                very_high_data.append(data[i])\n",
        "                very_high_label.append(labels[i])\n",
        "                very_high_prediction.append(predictions[i])\n",
        "                very_high_confidence.append(confidence[i])\n",
        "\n",
        "            elif conf >= bin2:\n",
        "                high_data.append(data[i])\n",
        "                high_label.append(labels[i])\n",
        "                high_prediction.append(predictions[i])\n",
        "                high_confidence.append(confidence[i])\n",
        "\n",
        "            elif conf >= bin3:\n",
        "                low_data.append(data[i])\n",
        "                low_label.append(labels[i])\n",
        "                low_prediction.append(predictions[i])\n",
        "                low_confidence.append(confidence[i])\n",
        "\n",
        "            elif conf >= bin4:\n",
        "                very_low_data.append(data[i])\n",
        "                very_low_label.append(labels[i])\n",
        "                very_low_prediction.append(predictions[i])\n",
        "                very_low_confidence.append(confidence[i])\n",
        "\n",
        "            else:\n",
        "                zero_data.append(data[i])\n",
        "                zero_label.append(labels[i])\n",
        "                zero_prediction.append(predictions[i])\n",
        "                zero_confidence.append(confidence[i])\n",
        "\n",
        "\n",
        "\n",
        "        self.write_CSV(very_high_data, very_high_label, very_high_prediction,  very_high_confidence , \"1\")\n",
        "        self.write_CSV(high_data, high_label, high_prediction,  high_confidence , \"2\")\n",
        "        self.write_CSV(low_data, low_label, low_prediction,  low_confidence , \"3\")\n",
        "        self.write_CSV(very_low_data, very_low_label, very_low_prediction,  very_low_confidence , \"4\")\n",
        "        self.write_CSV(zero_data, zero_label, zero_prediction, zero_confidence , \"5\")\n",
        "        self.write_opinion_words(data, positive_opinion_words, positive_negation_words, negative_opinion_words, negative_negation_words, labels, predictions, confidence)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8nUwRMrdnMM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C3sZqF4w0TqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    lexicalAnalyzer = LexicalAnalyzer()\n",
        "\n",
        "\n",
        "    data,label = lexicalAnalyzer.read_data()\n",
        "\n",
        "\n",
        "    predictions, pred_confidence_scores, positive_opinion_words, positive_negation_words, negative_opinion_words,  negative_negation_words = lexicalAnalyzer.classify_binary_dataset(data,label)\n",
        "\n"
      ],
      "metadata": {
        "id": "4fdWLZgxnQ23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexicalAnalyzer.distribute_predictions_into_bins(data,label,predictions, pred_confidence_scores, positive_opinion_words, positive_negation_words, negative_opinion_words, negative_negation_words)"
      ],
      "metadata": {
        "id": "usJmAyDYvdnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra"
      ],
      "metadata": {
        "id": "Sg_UWHAkz66o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "    def classify_ternary_dataset(self,data_english):\n",
        "\n",
        "        prediction_confidence_scores = []\n",
        "\n",
        "        polarity_scores = []\n",
        "        positive_scores = []\n",
        "        negative_scores = []\n",
        "\n",
        "        polarity_orientation = []\n",
        "\n",
        "        i = 0\n",
        "\n",
        "\n",
        "        for user_review in data_english:\n",
        "            #print(i, user_review)\n",
        "            sentiments = []\n",
        "            user_review = self.preprocess_data(user_review)\n",
        "            #user_review = stem_data(user_review)\n",
        "            user_review = self.split_review_text(user_review)\n",
        "            #tokens = [token.text for token in s if not token.is_stop]\n",
        "\n",
        "            total_score = 0\n",
        "            if i % 50 == 0:\n",
        "                print(\"-------------------------------------  \", i, len(user_review))\n",
        "\n",
        "            #print(\"-------------------------------------  \", i, user_review , len(user_review))\n",
        "            total_aspect_term = 0\n",
        "            total_positive_score = 0\n",
        "            total_negative_score = 0\n",
        "\n",
        "            for sentence in user_review:\n",
        "                tokens = nlp_english(sentence)\n",
        "                #print(\">> \",tokens)\n",
        "                aspect_sentence = []\n",
        "                #text = word_tokenize(sentence)\n",
        "                #print(\"## ##\",nltk.pos_tag(text))\n",
        "\n",
        "                for token in tokens:\n",
        "                    # if not token.is_stop:\n",
        "                    #if  token.dep_ == 'nsubj' or  token.dep_ == 'amod' or token.pos_ == 'ADJ':\n",
        "                    if token.dep_ == 'nsubj' or token.dep_ == 'neg'  or token.dep_ == 'advmod' or token.dep_ == 'ROOT' or token.dep_ == 'compound' or token.pos_ == 'ADJ' or token.pos_ == 'NOUN' or token.text == 'could' or token.text == 'must' or token.text == 'should':\n",
        "                       #print(token.text, token.dep_,  token.pos_,  [child for child in token.children])\n",
        "                       aspect_sentence.append(token.text)\n",
        "\n",
        "                #print(\"aspect_sentence\",aspect_sentence)\n",
        "                if len(aspect_sentence) >= 1:\n",
        "\n",
        "                    sentiments.append(aspect_sentence)\n",
        "\n",
        "                    aspect_sentence = self.remove_pronoun(aspect_sentence)\n",
        "\n",
        "\n",
        "                    if self.does_contain_adjective(aspect_sentence) == True:\n",
        "                        #removed += 1\n",
        "                       # print(\"*******\" , aspect_sentence)\n",
        "\n",
        "                        score, positive,negative = self.get_polarity_score(aspect_sentence)\n",
        "                        #print(\"-- \",i, score, \"Pos: \",positive, \"Neg: \",negative)\n",
        "\n",
        "                        total_positive_score += positive\n",
        "                        total_negative_score += negative\n",
        "                        #print (\"Score:   \", score)\n",
        "                       # score += get_polarity_score_sentic_net(aspect_sentence)\n",
        "\n",
        "                        negation_score = self.get_negation_score(aspect_sentence)\n",
        "                        if abs(negation_score) > 0 :\n",
        "                            score  +=  negation_score\n",
        "                            if negation_score < 0:\n",
        "                                total_positive_score -= 1\n",
        "                                total_negative_score += 1\n",
        "                            else:\n",
        "                                total_negative_score -= 1\n",
        "                                total_positive_score += 1\n",
        "\n",
        "                            #print (\"Neg: \",score,  aspect_sentence, negation_score)\n",
        "                        total_score += score\n",
        "                        total_score += self.get_comparison_score(aspect_sentence)\n",
        "                        total_negative_score -=  self.get_comparison_score(aspect_sentence)\n",
        "                        #if (total_score == 0):\n",
        "                            #total_score = Vader_Lexicon(aspect_sentence)\n",
        "\n",
        "                #(\"##### Total Score:  \", total_score )\n",
        "\n",
        "                total_aspect_term += len(aspect_sentence)\n",
        "\n",
        "            i += 1\n",
        "\n",
        "           # print(\"$$$$ ----total score : \", total_score, predicted_label)\n",
        "\n",
        "            polarity_scores.append(total_score)\n",
        "            positive_scores.append(total_positive_score)\n",
        "            negative_scores.append(total_negative_score)\n",
        "\n",
        "            prediction_confidence = 0\n",
        "            if total_score != 0:\n",
        "                total_positive_negative = total_positive_score + total_negative_score\n",
        "                prediction_confidence =  float(abs(total_positive_score - total_negative_score)/total_positive_negative)\n",
        "\n",
        "            prediction_confidence_scores.append(prediction_confidence)\n",
        "\n",
        "\n",
        "        return positive_scores,negative_scores, polarity_scores, prediction_confidence_scores\n",
        "\n",
        "    #----------Dataset----------\n",
        "    def distribute_predictions_into_bins(self, data, labels, predictions, confidence, positive_opinion_words, negative_opinion_words):\n",
        "\n",
        "        print(\"$$$$\")\n",
        "        n_conf = np.array(confidence)\n",
        "\n",
        "        thr = np.mean(n_conf) + 0.5 * np.std(n_conf)\n",
        "\n",
        "        print(np.mean(n_conf), thr)\n",
        "\n",
        "\n",
        "\n",
        "        bin1 = thr\n",
        "        bin2 = thr - 0.5 * np.std(n_conf)\n",
        "        bin3 = thr - np.std(n_conf)\n",
        "        bin4 = 0.0001\n",
        "\n",
        "        print(\"Bin Threshold\", bin1, bin2,bin3,bin4)\n",
        "\n",
        "        #return\n",
        "\n",
        "        very_high_data = []\n",
        "        very_high_label = []\n",
        "        very_high_prediction = []\n",
        "        very_high_confidence = []\n",
        "\n",
        "        high_data = []\n",
        "        high_label = []\n",
        "        high_prediction = []\n",
        "        high_confidence = []\n",
        "\n",
        "        low_data = []\n",
        "        low_label = []\n",
        "        low_prediction = []\n",
        "        low_confidence = []\n",
        "\n",
        "\n",
        "        very_low_data = []\n",
        "        very_low_label = []\n",
        "        very_low_prediction = []\n",
        "        very_low_confidence = []\n",
        "\n",
        "        zero_data = []\n",
        "        zero_label = []\n",
        "        zero_prediction = []\n",
        "        zero_confidence = []\n",
        "\n",
        "        print(\"---->>> \",len(data), len(confidence))\n",
        "        #return\n",
        "\n",
        "        for i in range(len(confidence)):\n",
        "            conf = confidence[i]\n",
        "            if conf >= bin1:\n",
        "                very_high_data.append(data[i])\n",
        "                very_high_label.append(labels[i])\n",
        "                very_high_prediction.append(predictions[i])\n",
        "                very_high_confidence.append(confidence[i])\n",
        "\n",
        "            elif conf >= bin2:\n",
        "                high_data.append(data[i])\n",
        "                high_label.append(labels[i])\n",
        "                high_prediction.append(predictions[i])\n",
        "                high_confidence.append(confidence[i])\n",
        "\n",
        "            elif conf >= bin3:\n",
        "                low_data.append(data[i])\n",
        "                low_label.append(labels[i])\n",
        "                low_prediction.append(predictions[i])\n",
        "                low_confidence.append(confidence[i])\n",
        "\n",
        "            elif conf >= bin4:\n",
        "                very_low_data.append(data[i])\n",
        "                very_low_label.append(labels[i])\n",
        "                very_low_prediction.append(predictions[i])\n",
        "                very_low_confidence.append(confidence[i])\n",
        "\n",
        "            else:\n",
        "                zero_data.append(data[i])\n",
        "                zero_label.append(labels[i])\n",
        "                zero_prediction.append(predictions[i])\n",
        "                zero_confidence.append(confidence[i])\n",
        "\n",
        "\n",
        "\n",
        "        self.write_CSV(very_high_data, very_high_label, very_high_prediction,  very_high_confidence , \"1\")\n",
        "        self.write_CSV(high_data, high_label, high_prediction,  high_confidence , \"2\")\n",
        "        self.write_CSV(low_data, low_label, low_prediction,  low_confidence , \"3\")\n",
        "        self.write_CSV(very_low_data, very_low_label, very_low_prediction,  very_low_confidence , \"4\")\n",
        "        self.write_CSV(zero_data, zero_label, zero_prediction, zero_confidence , \"5\")\n",
        "        self.write_opinion_words(data, positive_opinion_words, negative_opinion_words, positive_negation_words, negative_negation_words, labels, predictions, confidence)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Misi6_vNz-S_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}